# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16_tDKEEnYYOPMNLevyqvwbH1CxK47KuN
"""


# Import required libraries
import praw
from google import genai
import random
import whisper
import subprocess
import os
import re
import numpy as np
import soundfile as sf
from IPython.display import display, Audio
import torch
from diffusers import StableDiffusionPipeline
import time
from tqdm import tqdm
from PIL import Image, ImageDraw, ImageFont

# ===== CELL 2: API CREDENTIALS =====
# Reddit API credentials
reddit = praw.Reddit(
    client_id="Jf3jkA3Y0dBCfluYvS8aVw",
    client_secret="1dWKIP6ME7FBR66motXS6273rkkf0g",
    user_agent="Horror Stories by Wear_Severe"
)

# Initialize Gemini API
client = genai.Client(api_key="AIzaSyD_vBSluRNPI6z7JoKfl67M6D3DCq4l0NI")

# ===== CELL 3: CONSTANTS =====
# Horror subreddits
HORROR_SUBREDDITS = ["nosleep", "shortscarystories", "creepypasta", "LetsNotMeet"]

# Horror themes for filtering and prompting
HORROR_THEMES = [
    "paranormal", "ghost", "haunting", "demon", "possession",
    "monster", "creature", "stalker", "serial killer", "unexplained",
    "ritual", "cult", "ancient evil", "cursed", "shadow people",
    "sleep paralysis", "night terror", "abandoned", "forest", "cabin",
    "basement", "attic", "mirror", "doppelganger", "entity"
]

# Global variable for Stable Diffusion model
sd_pipeline = None

# ===== CELL 4: STABLE DIFFUSION INITIALIZATION =====
def initialize_stable_diffusion():
    """Initialize the Stable Diffusion model"""
    global sd_pipeline

    # Load Stable Diffusion 2.1 pipeline
    model_id = "stabilityai/stable-diffusion-2-1"
    sd_pipeline = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        use_safetensors=True
    )

    # Enable attention slicing for better memory efficiency
    sd_pipeline.enable_attention_slicing()

    # Move to GPU if available
    if torch.cuda.is_available():
        sd_pipeline = sd_pipeline.to("cuda")

    print("âœ… Stable Diffusion 2.1 model initialized")
    return sd_pipeline

# Initialize Stable Diffusion
initialize_stable_diffusion()

# ===== CELL 4: STABLE DIFFUSION INITIALIZATION =====
from diffusers import StableDiffusionXLPipeline
import torch

def initialize_stable_diffusion():
    """Initialize the Stable Diffusion XL model"""
    global sd_pipeline

    # Load Stable Diffusion XL pipeline
    model_id = "stabilityai/stable-diffusion-xl-base-1.0"
    sd_pipeline = StableDiffusionXLPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        use_safetensors=True
    )

    # Enable attention slicing for better memory efficiency
    sd_pipeline.enable_attention_slicing()

    # Move to GPU if available
    if torch.cuda.is_available():
        sd_pipeline = sd_pipeline.to("cuda")

    print("âœ… Stable Diffusion XL model initialized")
    return sd_pipeline

# Initialize Stable Diffusion XL
initialize_stable_diffusion()

# ===== CELL 5: SCRIPT GENERATION (FETCH AND ENHANCE STORY) =====
def fetch_and_enhance_nosleep_story():
    """Fetch a story from r/nosleep and enhance it using Gemini"""

    # Fetch stories from r/nosleep
    subreddit = reddit.subreddit("nosleep")
    posts = list(subreddit.top("month", limit=100))

    # Create a prompt for story selection
    post_titles = "\n".join([f"{i+1}. {post.title}" for i, post in enumerate(posts)])

    selection_prompt = """
    You are a professional horror story curator. Select ONE story from the following list
    that has the strongest potential for a cinematic narrative. Look for:
    - Clear narrative structure
    - Strong character development
    - Unique and original premise
    - Potential for visual storytelling

    Available stories:
    {titles}

    Return only the number of your selection (1-20).
    """.format(titles=post_titles)

    # Get story selection from Gemini Flash
    response = client.models.generate_content(
        model="gemini-2.0-flash",  # Using Flash model
        contents=selection_prompt
    ).text
    story_index = int(response.strip()) - 1
    chosen_story = posts[story_index]

    # Create enhanced podcast-style prompt
    enhancement_prompt = """
    Transform this horror story into a compelling podcast-style narrative similar to 'Lore'.
    Focus on atmospheric storytelling with these elements:

    STYLE:
    - Use conversational, intimate tone
    - Add historical or cultural context where relevant
    - Build tension through pacing and revelation
    - Include sensory details and environmental descriptions
    - Use rhetorical questions to engage listeners

    STRUCTURE:
    1. Opening Hook:
    - Start with a universal truth or historical fact
    - Connect it to the story's theme

    2. Core Narrative:
    - Weave in background details naturally
    - Build tension progressively
    - Use descriptive language for atmosphere
    - Include psychological elements

    3. Resolution:
    - Deliver a powerful climax
    - Leave lingering questions
    - Connect back to opening theme

    IMPORTANT:
    - Remove any Reddit-specific references
    - Skip meta commentary or formatting
    - Focus purely on the narrative
    - Use natural pauses and breaks
    - Create vivid imagery through words

    Original Story: {content}

    Rewrite as a compelling horror podcast episode that grips listeners from start to finish.
    """.format(content=chosen_story.selftext[:7000])

    # Get enhanced story from Gemini Flash
    enhanced_story = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=enhancement_prompt
    ).text

    # Clean up the enhanced story to remove any markdown or formatting
    enhanced_story = re.sub(r'#.*?\n', '', enhanced_story)  # Remove headers
    enhanced_story = re.sub(r'\*\*.*?\*\*', '', enhanced_story)  # Remove bold
    enhanced_story = re.sub(r'\*.*?\*', '', enhanced_story)  # Remove italics
    enhanced_story = re.sub(r'\n\s*\n\s*\n', '\n\n', enhanced_story)  # Fix spacing

    return {
        'title': chosen_story.title,
        'original': chosen_story.selftext,
        'enhanced': enhanced_story.strip()
    }


story_data = fetch_and_enhance_nosleep_story() # The indentation of this line was fixed to align with the function definition.
print(story_data['enhanced'])

def generate_image_prompts(story_text, num_prompts=20, style="neutral"):
    """Generate Stable Diffusion XL optimized prompts using Gemini based on story context

    Args:
        story_text (str): The story text to generate prompts from
        num_prompts (int): Number of prompts to generate
        style (str): Visual style direction ("realistic", "cinematic", "artistic", or "neutral")
    """
    # SDXL-optimized style guidance with stronger descriptors
    style_guidance = {
        "realistic": "photorealistic, intricate details, natural lighting, cinematic photography, 8k resolution, dramatic composition",
        "cinematic": "cinematic composition, dramatic lighting, film grain, anamorphic lens effect, professional cinematography, color grading, depth of field",
        "artistic": "digital art, stylized, vibrant colors, dramatic composition, concept art, trending on artstation, by Greg Rutkowski and ZdzisÅ‚aw BeksiÅ„ski",
        "neutral": "balanced composition, masterful photography, perfect exposure, selective focus, attention-grabbing depth of field, highly atmospheric"
    }

    # Use default if style not in options
    style_desc = style_guidance.get(style, style_guidance["neutral"])

    prompt = f"""
    You are a horror visual director specialized in creating prompts for Stable Diffusion XL. Create {num_prompts} vivid image prompts for key scenes from this story.

    Follow these specific requirements:
    1. Focus on atmospheric moments with strong emotional impact
    2. Include specific visual elements and details from the story
    3. Use highly descriptive language about lighting, textures, atmosphere and mood
    4. Structure prompts in this pattern: [main subject], [setting details], [atmospheric elements], [technical photography terms], [stylistic elements]
    5. Include terms like "highly detailed", "intricate", "atmospheric", "dramatic lighting", "8k resolution"
    6. Emphasize composition elements: "depth of field", "selective focus", "tilt-shift", "silhouette"
    7. Keep prompts under 400 characters but make them dense with visual information
    8. Focus on horror/dark atmosphere: include terms like "sinister", "haunting", "mysterious", "eerie", "unsettling"
    9. Style direction: {style_desc}

    Example format:
    1. Ancient cemetery at twilight, fog swirling around weathered gravestones, haunting atmosphere, highly detailed, atmospheric photography, tilt-shift, selective focus, dramatic lighting, muted colors, cinematic composition, 8k resolution
    2. Gothic mansion bedroom, decaying Victorian furniture, moonlight streaming through broken stained glass, sinister atmosphere, highly detailed, atmospheric photography, dramatic lighting, muted colors, cinematic composition, ornate details

    Story: {story_text[:6000]}  # Truncate to avoid token limits

    Return ONLY the numbered prompts (no extra text)
    """

    # Get response from Gemini
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=prompt
    ).text

    # Debug print the raw response
    print("Raw Gemini response:")
    print(response)

    # Improved prompt extraction with regex for better reliability
    import re
    prompt_pattern = r'^\s*\d+\.\s*(.+)$'
    prompts = []

    for line in response.strip().split('\n'):
        match = re.match(prompt_pattern, line.strip())
        if match:
            extracted_prompt = match.group(1).strip()
            # Enhance prompt with additional SDXL-specific terms
            enhanced_prompt = f"{extracted_prompt}, highly detailed, cinematic lighting, atmospheric, 8k resolution"
            prompts.append(enhanced_prompt)

    # Debug print the extracted prompts
    print(f"Extracted {len(prompts)} prompts:")
    for i, p in enumerate(prompts, 1):
        print(f"{i}. {p}")

    # Fallback prompts specifically designed for SDXL
    fallback_prompts = [
        "Dark hallway with flickering lights, shadows stretching across peeling wallpaper, sinister atmosphere, highly detailed, atmospheric photography, selective focus, dramatic lighting, cinematic composition, 8k resolution, muted colors",
        "Abandoned Victorian room with a single chair, moonlight casting long shadows through broken windows, eerie atmosphere, highly detailed, tilt-shift photography, selective focus, dramatic lighting, atmospheric, 8k resolution",
        "Antique mirror reflecting something unnatural in the darkness, ornate frame, dust particles floating in moonbeams, mysterious atmosphere, highly detailed, macro photography, selective focus, dramatic lighting, cinematic composition",
        "Fog-shrouded forest path at dusk with twisted trees forming haunting silhouettes, mist swirling around gnarled roots, sinister atmosphere, highly detailed, atmospheric photography, depth of field, dramatic lighting, 8k resolution",
        "Ancient basement with strange markings etched into stone walls, single light bulb casting eerie shadows, unsettling atmosphere, highly detailed, dark photography, selective focus, dramatic lighting, cinematic composition, 8k resolution"
    ]

    # If we have fewer than expected prompts, fill with sophisticated defaults
    if len(prompts) < num_prompts:
        needed = num_prompts - len(prompts)
        prompts.extend(fallback_prompts[:needed])

    return prompts[:num_prompts]

# Example usage:
prompts = generate_image_prompts(story_data['enhanced'], num_prompts=5, style="cinematic")
print("Generated SDXL-Optimized Prompts:", prompts)

# Make sure to actually use these prompts
if prompts:  # Check that we have prompts
    print("\nGenerating images from these prompts...")
    # If you have fewer than expected prompts, fill with defaults
    while len(prompts) < 20:
        prompts.append("Dark, atmospheric horror scene with subtle lighting and eerie mood, highly detailed, dramatic composition, atmospheric photography, cinematic lighting, 8k resolution")
else:
    print("WARNING: No prompts were generated. Using default prompts instead.")
    prompts = [
        "Gothic style abandoned mansion, mechanical battle-hardened figure silhouetted in doorway, dark digital fantasy high-tech world, sinister vibes, intimidating presence, highly detailed, atmospheric photography, dramatic lighting, ornate details, 8k resolution",

        "Dull highly detailed swamp forest flooded with murky water, highly atmospheric aesthetic photography, tilt-shift, selective focus, attention-grabbing depth of field, mist rising from water surface, eerie atmosphere, 8k resolution, cinematic lighting",

        "Haunted Victorian portrait of woman with long brunette hair, loose clothing with vintage style, light colored eyes staring directly at viewer, warm sunlight casting long shadows, silhouette style, sinister atmosphere, highly detailed photography",

        "Ancient cemetery at twilight, fog swirling around weathered gravestones, haunting atmosphere, highly detailed, atmospheric photography, tilt-shift, selective focus, dramatic lighting, muted colors, cinematic composition, 8k resolution",

        "Gothic mansion bedroom, decaying Victorian furniture, moonlight streaming through broken stained glass, sinister atmosphere, highly detailed, atmospheric photography, dramatic lighting, muted colors, cinematic composition, ornate details"
    ]

# Configuration constants
STYLE_GUIDANCE = {
    "realistic": "photorealistic, intricate details, natural lighting, cinematic photography, 8k resolution, dramatic composition",
    "cinematic": "cinematic composition, dramatic lighting, film grain, anamorphic lens effect, professional cinematography, color grading, depth of field",
    "artistic": "digital art, stylized, vibrant colors, dramatic composition, concept art, trending on artstation, by Greg Rutkowski and ZdzisÅ‚aw BeksiÅ„ski",
    "neutral": "balanced composition, masterful photography, perfect exposure, selective focus, attention-grabbing depth of field, highly atmospheric"
}

DEFAULT_PROMPTS = [
    "Dark hallway with flickering lights, shadows stretching across peeling wallpaper, sinister atmosphere",
    "Abandoned Victorian room with a single chair, moonlight casting long shadows through broken windows",
    "Antique mirror reflecting something unnatural in the darkness, ornate frame, dust particles floating in moonbeams",
    "Fog-shrouded forest path at dusk with twisted trees forming haunting silhouettes",
    "Ancient basement with strange markings etched into stone walls, single light bulb casting eerie shadows",
    "Decrepit hospital corridor with medical equipment scattered about, emergency lights flashing",
    "Rustic cabin interior with mysterious symbols carved into wooden walls, candlelight flickering",
    "Misty graveyard at midnight, ancient tombstones covered in twisted vines",
    "Abandoned library with books floating in mid-air, dust particles catching moonlight",
    "Old attic filled with covered furniture, strange shadows moving beneath sheets",
    "Victorian doll collection in glass cases, one doll turned to face the wrong direction",
    "Underground crypt with ancient sarcophagi, strange light emanating from cracks",
    "Derelict church interior, stained glass windows casting colored shadows",
    "Childhood bedroom transformed into something sinister, toys arranged unnaturally",
    "Endless corridor with doors stretching into darkness, one door slightly ajar",
    "Ancient ritual circle in forest clearing, strange symbols glowing faintly",
    "Vintage photography darkroom with eerie images developing themselves",
    "Haunted music room with grand piano playing itself in moonlight",
    "Medieval dungeon with mysterious alchemical equipment, green light glowing",
    "Ghost town main street at night, figures visible in windows"
]

def enhance_prompt(prompt):
    """Add standard enhancement terms to a prompt"""
    return f"{prompt}, highly detailed, cinematic lighting, atmospheric, 8k resolution"

def generate_image_prompts(story_text, num_prompts=20, style="neutral", custom_prompt_template=None):
    """Generate Stable Diffusion XL optimized prompts using Gemini based on story context

    Args:
        story_text (str): The story text to generate prompts from
        num_prompts (int): Number of prompts to generate
        style (str): Visual style direction ("realistic", "cinematic", "artistic", or "neutral")
        custom_prompt_template (str, optional): Custom template for the AI prompt
    """
    style_desc = STYLE_GUIDANCE.get(style, STYLE_GUIDANCE["neutral"])

    # Use custom template if provided, otherwise use default
    if not custom_prompt_template:
        custom_prompt_template = f"""
        You are a horror visual director specialized in creating prompts for Stable Diffusion XL. Create {num_prompts} vivid image prompts for key scenes from this story.

        Follow these specific requirements:
        1. Focus on atmospheric moments with strong emotional impact
        2. Include specific visual elements and details from the story
        3. Use highly descriptive language about lighting, textures, atmosphere and mood
        4. Structure prompts in this pattern: [main subject], [setting details], [atmospheric elements], [technical photography terms], [stylistic elements]
        5. Include terms like "highly detailed", "intricate", "atmospheric", "dramatic lighting", "8k resolution"
        6. Emphasize composition elements: "depth of field", "selective focus", "tilt-shift", "silhouette"
        7. Keep prompts under 400 characters but make them dense with visual information
        8. Focus on horror/dark atmosphere: include terms like "sinister", "haunting", "mysterious", "eerie", "unsettling"
        9. Style direction: {style_desc}

        Story: {story_text[:6000]}

        Return ONLY the numbered prompts (no extra text)
        """

    # Get response from Gemini
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=custom_prompt_template
    ).text

    # Extract prompts using regex
    import re
    prompt_pattern = r'^\s*\d+\.\s*(.+)$'
    prompts = []

    for line in response.strip().split('\n'):
        match = re.match(prompt_pattern, line.strip())
        if match:
            extracted_prompt = match.group(1).strip()
            prompts.append(enhance_prompt(extracted_prompt))

    # If we have fewer than expected prompts, fill with enhanced defaults
    if len(prompts) < num_prompts:
        needed = num_prompts - len(prompts)
        enhanced_defaults = [enhance_prompt(p) for p in DEFAULT_PROMPTS[:needed]]
        prompts.extend(enhanced_defaults)

    return prompts[:num_prompts]

# Example usage:
if __name__ == "__main__":
    # Test different configurations
    test_configs = [
        {"num_prompts": 20, "style": "cinematic"}

    ]

    for config in test_configs:
        prompts = generate_image_prompts(
            story_text=story_data['enhanced'],
            **config
        )
        print(f"\nGenerated {len(prompts)} prompts with style {config['style']}:")
        for i, prompt in enumerate(prompts, 1):
            print(f"{i}. {prompt}")

# ===== CELL 7: AUTOMATED IMAGE GENERATION SD 2.1 =====
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
import torch

# Configuration (hidden from user)
ASPECT_RATIO = (968, 544)  # 16:9 (540p)
AUTO_NEGATIVE_PROMPT = ("low quality, cartoon, 3D render, anime, text, watermark, "
                        "signature, blurry, deformed, extra fingers, mutated hands, "
                        "poor lighting, bad anatomy, low resolution, child, colorful")

def initialize_stable_diffusion():
    """Auto-configure Stable Diffusion for horror visuals"""
    global sd_pipeline

    model_id = "stabilityai/stable-diffusion-2-1"

    sd_pipeline = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        use_safetensors=True
    ).to("cuda" if torch.cuda.is_available() else "cpu")

    # Professional settings
    sd_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
        sd_pipeline.scheduler.config,
        algorithm_type="dpms+++",
        use_karras_sigmas=True
    )

    sd_pipeline.enable_attention_slicing()
    sd_pipeline.enable_model_cpu_offload()

    return sd_pipeline

def auto_generate_image(prompt):
    """Generate image with automatic enhancements"""
    enhanced_prompt = (
        f"{prompt}, hyperrealistic digital painting, 4k resolution, "
        "unreal engine 5, cinematic lighting, artstation trending, "
        "by Greg Rutkowski and ZdzisÅ‚aw BeksiÅ„ski, film grain"
    )

    image = sd_pipeline(
        prompt=enhanced_prompt,
        negative_prompt=AUTO_NEGATIVE_PROMPT,
        width=ASPECT_RATIO[0],
        height=ASPECT_RATIO[1],
        num_inference_steps=25,
        guidance_scale=10.0,
        generator=torch.Generator(device="cuda").manual_seed(int(time.time()))
    ).images[0]

    return image

def generate_story_images(story_data):
    """Fully automated image pipeline"""
    if sd_pipeline is None:
        initialize_stable_diffusion()

    # Auto-create output directory
    output_dir = "auto_images"
    os.makedirs(output_dir, exist_ok=True)

    # Generate prompts from story
    prompts = generate_image_prompts(story_data['enhanced'])

    # Generate images
    image_paths = []
    for idx, prompt in enumerate(prompts, 1):
        output_path = os.path.join(output_dir, f"scene_{idx}.png")
        auto_generate_image(prompt).save(output_path)
        image_paths.append(output_path)
        print(f"Generated {idx}/{len(prompts)} images")

    return image_paths

# Initialize once
if 'sd_pipeline' not in globals():
    initialize_stable_diffusion()

image_paths = generate_story_images(story_data)

# ===== CELL 7: AUTOMATED IMAGE GENERATION SDXL =====
from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler
import torch
import os
import time

# Configuration (hidden from user)
ASPECT_RATIO = (968, 544)  # 16:9 (540p)
AUTO_NEGATIVE_PROMPT = ("low quality, cartoon, 3D render, anime, text, watermark, "
                        "signature, blurry, deformed, extra fingers, mutated hands, "
                        "poor lighting, bad anatomy, low resolution, child, colorful")

def initialize_stable_diffusion():
    """Auto-configure Stable Diffusion XL for horror visuals"""
    global sd_pipeline

    model_id = "stabilityai/stable-diffusion-xl-base-1.0"

    sd_pipeline = StableDiffusionXLPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        use_safetensors=True
    ).to("cuda" if torch.cuda.is_available() else "cpu")

    # Professional settings
    sd_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
        sd_pipeline.scheduler.config,
        algorithm_type="dpms+++",
        use_karras_sigmas=True
    )

    sd_pipeline.enable_attention_slicing()
    # Note: SDXL might not have model_cpu_offload in some diffusers versions
    try:
        sd_pipeline.enable_model_cpu_offload()
    except:
        print("Model CPU offload not available for this version - continuing without it")

    return sd_pipeline

def auto_generate_image(prompt):
    """Generate image with automatic enhancements"""
    enhanced_prompt = (
        f"{prompt}, hyperrealistic digital painting, 4k resolution, "
        "unreal engine 5, cinematic lighting, artstation trending, "
        "by Greg Rutkowski and ZdzisÅ‚aw BeksiÅ„ski, film grain"
    )

    image = sd_pipeline(
        prompt=enhanced_prompt,
        negative_prompt=AUTO_NEGATIVE_PROMPT,
        width=ASPECT_RATIO[0],
        height=ASPECT_RATIO[1],
        num_inference_steps=25,
        guidance_scale=10.0,
        generator=torch.Generator(device="cuda").manual_seed(int(time.time()))
    ).images[0]

    return image

def generate_story_images(story_data):
    """Fully automated image pipeline"""
    if 'sd_pipeline' not in globals() or sd_pipeline is None:
        initialize_stable_diffusion()

    # Auto-create output directory
    output_dir = "auto_images"
    os.makedirs(output_dir, exist_ok=True)

    # Generate prompts from story
    prompts = generate_image_prompts(story_data['enhanced'])

    # Generate images
    image_paths = []
    for idx, prompt in enumerate(prompts, 1):
        output_path = os.path.join(output_dir, f"scene_{idx}.png")
        auto_generate_image(prompt).save(output_path)
        image_paths.append(output_path)
        print(f"Generated {idx}/{len(prompts)} images")

    return image_paths

# Initialize once
if 'sd_pipeline' not in globals():
    initialize_stable_diffusion()

image_paths = generate_story_images(story_data)

# ===== CELL 7: VOICE OVER SCRIPT GENERATION =====
def generate_voice_over_script(story_text):
    """Generate a voice-over script from the enhanced story"""
    # This function can be expanded to include specific formatting for voice-over
    return story_text

# Example usage:
voice_over_script = generate_voice_over_script(story_data['enhanced']) # Removed the extra space at the beginning of the line.
print(voice_over_script)

# ===== CELL 8: AUTOMATED VOICE OVER GENERATION =====
import numpy as np
import soundfile as sf
from kokoro import KPipeline
import os

def generate_horror_audio(story_text, output_dir="audio_output"):
    """Generate professional horror narration audio"""
    os.makedirs(output_dir, exist_ok=True)

    # Initialize pipeline with horror-optimized settings
    pipeline = KPipeline(lang_code='a')

    # Professional voice parameters for horror narration
    generator = pipeline(
        story_text,
        voice='af_bella',  # Special horror-optimized voice
        speed=0.85,             # Slower pace for tension building


    )

    # Generate and concatenate audio segments
    audio_data = np.concatenate([audio.numpy() for _, _, audio in generator])

    # Save high-quality audio file
    output_path = os.path.join(output_dir, "narration.wav")
    sf.write(output_path, audio_data, 24000)

    print(f"ðŸ”Š Professional horror narration generated: {output_path}")
    return output_path

# Example usage:
audio_path = generate_horror_audio(story_data['enhanced'])

# ===== CELL 9: SUBTITLE GENERATION =====
from transformers import pipeline
import whisper
from whisper.utils import WriteSRT

def generate_subtitles(audio_path, output_dir="subtitles"):
    """Generate subtitles using Whisper large-v3 model"""
    os.makedirs(output_dir, exist_ok=True)

    # Load Whisper model (optimized for horror narration)
    model = whisper.load_model("base")  # Use 'small' for better accuracy

    # Transcribe with timing info
    result = model.transcribe(audio_path,
                            verbose=False,
                            word_timestamps=True,
                            fp16=torch.cuda.is_available())

    # Save SRT file
    srt_path = os.path.join(output_dir, "subtitles.srt")
    with open(srt_path, "w", encoding="utf-8") as srt_file:
        writer = WriteSRT(output_dir)
        writer.write_result(result, srt_file)

    print(f"ðŸ“œ Subtitles generated: {srt_path}")
    return srt_path

# Example usage:
srt_path = generate_subtitles(audio_path)

import os
import re
import gc
import numpy as np
from moviepy.editor import *
from moviepy.video.compositing.transitions import crossfadein
from typing import List, Optional
import sys
import contextlib

# Configure ImageMagick
try:
    from moviepy.config import change_settings
    # Try to use ImageMagick binary directly
    change_settings({"IMAGEMAGICK_BINARY": "convert"})
except:
    # Fallback paths for ImageMagick
    possible_paths = [
        "C:\\Program Files\\ImageMagick-7.0.10-Q16\\convert.exe",  # Windows default
        "C:\\Program Files\\ImageMagick-7.1.1-Q16-HDRI\\convert.exe",  # Newer version
        "/usr/bin/convert",  # Linux/Mac
        "convert"  # Try system path
    ]

    for path in possible_paths:
        if os.path.exists(path):
            change_settings({"IMAGEMAGICK_BINARY": path})
            break

def db_to_amplitude(db: float) -> float:
    """Convert decibels to amplitude ratio"""
    return 10 ** (db / 20)

# Constants for optimization
BATCH_SIZE = 2  # Process images in smaller batches
MAX_DIMENSION = 1920  # Increased for full HD
JPEG_QUALITY = 85  # Quality for intermediate saves
TRANSITION_DURATION = 1.0  # Duration of dissolve transition
BG_MUSIC_DB = -17  # Background music level in dB

@contextlib.contextmanager
def suppress_output():
    """Suppress output temporarily."""
    with open(os.devnull, 'w') as devnull:
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        try:
            sys.stdout = devnull
            sys.stderr = devnull
            yield
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr

class VideoGenerator:
    def __init__(self, output_dir: str = "/content/output"):
        self.output_dir = output_dir
        self.temp_dir = os.path.join(output_dir, "temp")
        os.makedirs(self.temp_dir, exist_ok=True)

    def cleanup(self):
        """Clean temporary files"""
        import shutil
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    @staticmethod
    def optimize_image(img_clip: ImageClip) -> ImageClip:
        """Optimize image clip for memory usage and ensure full screen cover"""
        target_aspect = 16/9  # HD video aspect ratio
        current_size = img_clip.size
        current_aspect = current_size[0] / current_size[1]

        if current_aspect > target_aspect:
            # Image is wider than 16:9
            new_width = int(current_size[1] * target_aspect)
            x_center = (current_size[0] - new_width) // 2
            img_clip = img_clip.crop(x1=x_center, width=new_width)
        else:
            # Image is taller than 16:9
            new_height = int(current_size[0] / target_aspect)
            y_center = (current_size[1] - new_height) // 2
            img_clip = img_clip.crop(y1=y_center, height=new_height)

        return img_clip.resize(width=1920, height=1080)  # Full HD resolution

    def create_final_video(self, image_paths: List[str], audio_path: str,
                          title: str, srt_path: Optional[str] = None) -> str:
        try:
            print("Starting video creation...")

            # Get audio duration first for image loop calculation
            audio = AudioFileClip(audio_path)
            total_duration = audio.duration

            # Process images with fixed duration and smooth zoom
            CLIP_DURATION = 10  # 10 seconds per image
            base_clips = []

            for img_path in image_paths:
                if not os.path.exists(img_path):
                    continue

                # Create clip with smooth zoom from 1.0 to 1.3 scale
                clip = (ImageClip(img_path)
                       .set_duration(CLIP_DURATION)
                       .resize(lambda t: 1 + (0.3 * t / CLIP_DURATION))  # Smooth zoom
                       .set_position('center'))

                base_clips.append(clip)

            # Calculate how many complete sets of images we need
            total_needed_duration = total_duration
            full_sequence_duration = len(base_clips) * CLIP_DURATION

            # If we need more duration, duplicate the clips
            if total_needed_duration > full_sequence_duration:
                repeats = int(np.ceil(total_needed_duration / full_sequence_duration))
                base_clips = base_clips * repeats

            # Trim to exact duration needed
            clips_needed = int(np.ceil(total_duration / CLIP_DURATION))
            base_clips = base_clips[:clips_needed]

            # Apply transitions
            video_fx_list = [base_clips[0]]
            current_time = 0

            for i in range(1, len(base_clips)):
                current_time += CLIP_DURATION - TRANSITION_DURATION
                clip_with_fade = base_clips[i].set_start(current_time).crossfadein(TRANSITION_DURATION)
                video_fx_list.append(clip_with_fade)

            print("Combining video clips...")
            video = CompositeVideoClip(video_fx_list).resize(width=1920, height=1080)

            # Add subtitles if available with enhanced styling
            if srt_path and os.path.exists(srt_path):
                from moviepy.video.tools.subtitles import SubtitlesClip
                try:
                    # Enhanced subtitle style with bold text
                    generator = lambda txt: TextClip(
                        txt.upper(),  # Uppercase for better visibility
                        font='Arial-Bold',  # Bold font
                        fontsize=48,
                        color='white',
                        stroke_color='black',
                        stroke_width=2,
                        method='caption',
                        size=(video.w * 0.8, None),
                        align='center'
                    )
                except:
                    # Fallback with bold styling
                    generator = lambda txt: TextClip(
                        f"<b>{txt}</b>",  # HTML bold tags
                        font='Arial',
                        fontsize=48,
                        color='white',
                        method='label',
                        size=(video.w * 0.8, None)
                    )

                subs = SubtitlesClip(srt_path, generator)
                # Place subtitles in true center with slight bottom offset
                video = CompositeVideoClip([
                    video,
                    subs.set_position(('center', 'center'))
                ])

            print("Processing audio...")
            # Process audio with specific volume
            if os.path.exists("/content/ambient.mp3"):
                bg_music = (AudioFileClip("/content/ambient.mp3")
                          .volumex(db_to_amplitude(BG_MUSIC_DB)))

                if bg_music.duration < audio.duration:
                    bg_music = afx.audio_loop(bg_music, duration=audio.duration)
                else:
                    bg_music = bg_music.subclip(0, audio.duration)

                final_audio = CompositeAudioClip([audio, bg_music])
            else:
                final_audio = audio

            video = video.set_audio(final_audio)

            print("Rendering final video...")
            output_file = os.path.join(self.output_dir, f"{title}.mp4")
            with suppress_output():
                video.write_videofile(
                    output_file,
                    fps=24,
                    codec='libx264',
                    audio_codec='aac',
                    bitrate='4000k',  # Higher bitrate for better quality
                    threads=4,
                    preset='faster',  # Balance between speed and quality
                    ffmpeg_params=['-crf', '23'],  # Better quality
                    logger=None  # Disable moviepy's progress bars
                )

            # Cleanup
            video.close()
            audio.close()
            self.cleanup()
            gc.collect()

            print("Video creation completed successfully.")
            return output_file

        except Exception as e:
            print(f"Error: {str(e)}")
            self.cleanup()
            return self._create_simple_video(image_paths[0], audio_path, title)

    def _create_simple_video(self, image_path: str, audio_path: str, title: str) -> str:
        """Fallback simple video creation"""
        try:
            clip = ImageClip(image_path)
            clip = self.optimize_image(clip)
            audio = AudioFileClip(audio_path)
            clip = clip.set_duration(audio.duration).set_audio(audio)

            output_file = os.path.join(self.output_dir, f"simple_{title}.mp4")
            with suppress_output():
                clip.write_videofile(
                    output_file,
                    fps=24,
                    codec='libx264',
                    preset='ultrafast',
                    ffmpeg_params=['-crf', '30']
                )

            clip.close()
            audio.close()
            return output_file

        except Exception as e:
            print(f"Fallback error: {str(e)}")
            return ""

# Main execution
if __name__ == "__main__":
    # Construct image paths
    image_paths = [f"/content/auto_images/scene_{i}.png" for i in range(1, 6)]

    # Set audio path
    audio_path = "/content/audio_output/narration.wav"

    # Set subtitle path
    srt_path = "/content/subtitles/subtitles.srt"

    # Create output directory
    output_dir = "/content/output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Create the video
    generator = VideoGenerator(output_dir=output_dir)
    video_path = generator.create_final_video(
        image_paths=image_paths,
        audio_path=audio_path,
        title="The_Haunting",
        srt_path=srt_path
    )

    if video_path and os.path.exists(video_path):
        print(f"Video creation successful! Video saved to: {video_path}")

        # Display the video in Colab
        from IPython.display import HTML
        from base64 import b64encode

        mp4 = open(video_path, 'rb').read()
        data_url = f"data:video/mp4;base64,{b64encode(mp4).decode()}"
        display(HTML(f"""
        <video width="640" height="360" controls>
            <source src="{data_url}" type="video/mp4">
        </video>
        """))